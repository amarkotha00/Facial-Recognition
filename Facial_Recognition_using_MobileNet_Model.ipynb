{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarkotha00/Facial-Recognition/blob/main/Facial_Recognition_using_MobileNet_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4VHWbn744af"
      },
      "source": [
        "### Pre_Processing  - Face Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QrI8Tuv44ah"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuKQyd1w44aj",
        "outputId": "52a7cc41-c180-4638-e3d0-bf1259bc26ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Name\n",
            "Amar\n"
          ]
        }
      ],
      "source": [
        "name=input(\"Enter your Name\\n\")\n",
        "\n",
        "#Enter your Name to check if you are a Registered User else Register now.\n",
        "\n",
        "l = os.listdir('datasets/train/')\n",
        "if name not in l :\n",
        "    l.append(name)\n",
        "    l.sort()\n",
        "    os.mkdir('datasets/train/'+name)\n",
        "    os.mkdir('datasets/validation/'+name)\n",
        "    p=1\n",
        "else:\n",
        "    p=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "86sDfghN44ak",
        "outputId": "58b9d033-2b39-4b7f-c656-1a088be4132b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-24-49fc47dea773>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if faces is ():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a Registered User\n",
            "Registered Users are\n",
            "['Abhishek', 'Adithya Aras', 'Akshay', 'Amar']\n"
          ]
        }
      ],
      "source": [
        "#We use Haarcascade Classifier for Pre-Processing of the images.\n",
        "\n",
        "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Load functions\n",
        "def face_extractor(img):\n",
        "    # Function detects faces and returns the cropped face\n",
        "    # If no face detected, it returns the input image\n",
        "    \n",
        "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(img, 1.03, 5)\n",
        "    \n",
        "    if faces is ():\n",
        "        return None\n",
        "    \n",
        "    # Crop all faces found\n",
        "    for (x,y,w,h) in faces:\n",
        "        cropped_face = img[y:y+h, x:x+w]\n",
        "\n",
        "    return cropped_face\n",
        "\n",
        "# Initialize Webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "count = 0\n",
        "\n",
        "#If new user, Capture 100 images of the new face for Train DATA.\n",
        "    \n",
        "if p==1 :\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if face_extractor(frame) is not None:\n",
        "            count += 1\n",
        "            face = cv2.resize(face_extractor(frame), (200, 200))\n",
        "            #face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Save file in specified directory with unique name\n",
        "            file_name_path = 'datasets/train/'+name+'/image_' + str(count) + '.jpg'\n",
        "            cv2.imwrite(file_name_path, face)\n",
        "\n",
        "            # Put count on images and display live count\n",
        "            cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
        "            cv2.imshow('Face Cropper', face)\n",
        "        \n",
        "        else:\n",
        "            print(\"Face not found\")\n",
        "            pass\n",
        "\n",
        "        if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
        "            break\n",
        "        \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()      \n",
        "    print(\"Collecting Samples Complete\")\n",
        "\n",
        "else :\n",
        "    print(\"You are a Registered User\")\n",
        "\n",
        "print(\"Registered Users are\")\n",
        "print(l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlXXjLp-44al",
        "outputId": "f428ac64-0ecd-4d46-b303-fbfa7a9b748b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-25-383248d05455>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if faces is ():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a Registered User\n",
            "Registered Users are\n",
            "['Abhishek', 'Adithya Aras', 'Akshay', 'Amar']\n"
          ]
        }
      ],
      "source": [
        "#We use Haarcascade Classifier for Pre-Processing of the images.\n",
        "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Load functions\n",
        "def face_extractor(img):\n",
        "    # Function detects faces and returns the cropped face\n",
        "    # If no face detected, it returns the input image\n",
        "    \n",
        "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(img, 1.03, 5)\n",
        "    \n",
        "    if faces is ():\n",
        "        return None\n",
        "    \n",
        "    # Crop all faces found\n",
        "    for (x,y,w,h) in faces:\n",
        "        cropped_face = img[y:y+h, x:x+w]\n",
        "\n",
        "    return cropped_face\n",
        "\n",
        "# Initialize Webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "count = 0\n",
        "\n",
        "#If new user, Capture 20 images of the new face for Validation DATA.\n",
        "    \n",
        "if p==1 :\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if face_extractor(frame) is not None:\n",
        "            count += 1\n",
        "            face = cv2.resize(face_extractor(frame), (200, 200))\n",
        "            #face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Save file in specified directory with unique name\n",
        "            file_name_path = 'datasets/validation/'+name+'/image_' + str(count) + '.jpg'\n",
        "            cv2.imwrite(file_name_path, face)\n",
        "\n",
        "            # Put count on images and display live count\n",
        "            cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
        "            cv2.imshow('Face Cropper', face)\n",
        "        \n",
        "        else:\n",
        "            print(\"Face not found\")\n",
        "            pass\n",
        "\n",
        "        if cv2.waitKey(1) == 13 or count == 20: #13 is the Enter Key\n",
        "            break\n",
        "        \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()      \n",
        "    print(\"Collecting Samples Complete\")\n",
        "\n",
        "else :\n",
        "    print(\"You are a Registered User\")\n",
        "\n",
        "print(\"Registered Users are\")\n",
        "print(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qyIQQA_44al"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmINBbyG44am"
      },
      "source": [
        "### Freeze all layers except the top 4, as we'll only be training the top 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMHdW-yW44am",
        "outputId": "bde4301a-0962-468e-a450-4c95692545e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 BatchNormalization False\n",
            "3 ReLU False\n",
            "4 DepthwiseConv2D False\n",
            "5 BatchNormalization False\n",
            "6 ReLU False\n",
            "7 Conv2D False\n",
            "8 BatchNormalization False\n",
            "9 ReLU False\n",
            "10 ZeroPadding2D False\n",
            "11 DepthwiseConv2D False\n",
            "12 BatchNormalization False\n",
            "13 ReLU False\n",
            "14 Conv2D False\n",
            "15 BatchNormalization False\n",
            "16 ReLU False\n",
            "17 DepthwiseConv2D False\n",
            "18 BatchNormalization False\n",
            "19 ReLU False\n",
            "20 Conv2D False\n",
            "21 BatchNormalization False\n",
            "22 ReLU False\n",
            "23 ZeroPadding2D False\n",
            "24 DepthwiseConv2D False\n",
            "25 BatchNormalization False\n",
            "26 ReLU False\n",
            "27 Conv2D False\n",
            "28 BatchNormalization False\n",
            "29 ReLU False\n",
            "30 DepthwiseConv2D False\n",
            "31 BatchNormalization False\n",
            "32 ReLU False\n",
            "33 Conv2D False\n",
            "34 BatchNormalization False\n",
            "35 ReLU False\n",
            "36 ZeroPadding2D False\n",
            "37 DepthwiseConv2D False\n",
            "38 BatchNormalization False\n",
            "39 ReLU False\n",
            "40 Conv2D False\n",
            "41 BatchNormalization False\n",
            "42 ReLU False\n",
            "43 DepthwiseConv2D False\n",
            "44 BatchNormalization False\n",
            "45 ReLU False\n",
            "46 Conv2D False\n",
            "47 BatchNormalization False\n",
            "48 ReLU False\n",
            "49 DepthwiseConv2D False\n",
            "50 BatchNormalization False\n",
            "51 ReLU False\n",
            "52 Conv2D False\n",
            "53 BatchNormalization False\n",
            "54 ReLU False\n",
            "55 DepthwiseConv2D False\n",
            "56 BatchNormalization False\n",
            "57 ReLU False\n",
            "58 Conv2D False\n",
            "59 BatchNormalization False\n",
            "60 ReLU False\n",
            "61 DepthwiseConv2D False\n",
            "62 BatchNormalization False\n",
            "63 ReLU False\n",
            "64 Conv2D False\n",
            "65 BatchNormalization False\n",
            "66 ReLU False\n",
            "67 DepthwiseConv2D False\n",
            "68 BatchNormalization False\n",
            "69 ReLU False\n",
            "70 Conv2D False\n",
            "71 BatchNormalization False\n",
            "72 ReLU False\n",
            "73 ZeroPadding2D False\n",
            "74 DepthwiseConv2D False\n",
            "75 BatchNormalization False\n",
            "76 ReLU False\n",
            "77 Conv2D False\n",
            "78 BatchNormalization False\n",
            "79 ReLU False\n",
            "80 DepthwiseConv2D False\n",
            "81 BatchNormalization False\n",
            "82 ReLU False\n",
            "83 Conv2D False\n",
            "84 BatchNormalization False\n",
            "85 ReLU False\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import MobileNet\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows, img_cols = 224, 224  \n",
        "\n",
        "X = MobileNet(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "# Re-loads the VGG model without the top or FC layers\n",
        "\n",
        "\n",
        "# Here we freeze the last 4 layers \n",
        "# Layers are set to trainable as True by default\n",
        "for layer in X.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(X.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhch24XU44am"
      },
      "source": [
        "### Function that returns our FC Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xudcgeO44am"
      },
      "outputs": [],
      "source": [
        "def add_layer(bottom_model, num_classes):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "\n",
        "    top_model = bottom_model.output\n",
        "    top_model = GlobalAveragePooling2D()(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dense(512,activation='relu')(top_model)\n",
        "    top_model = Dense(num_classes,activation='softmax')(top_model)\n",
        "    return top_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp1aVXxw44an"
      },
      "source": [
        "### Add  FC Head back onto MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ejcfpdj-44an",
        "outputId": "c2c92b2f-66d6-4364-9743-3306bf27193c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 5,854,916\n",
            "Trainable params: 2,626,052\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "# Set our class number to number of Registered Users\n",
        "num_classes = len(l)\n",
        "\n",
        "FC_Head = add_layer(X, num_classes)\n",
        "\n",
        "model = Model(inputs = X.input, outputs = FC_Head)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzDXFVPn44an"
      },
      "source": [
        "### Loading our Face Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6vXr_3q44an",
        "outputId": "d1d9e7d4-1c29-4e22-824d-dc936adfc897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 293 images belonging to 4 classes.\n",
            "Found 72 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_dir = 'datasets/train'\n",
        "validation_data_dir = 'datasets/validation'\n",
        "\n",
        "# Let's use some data augmentaiton \n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=45,\n",
        "      width_shift_range=0.3,\n",
        "      height_shift_range=0.3,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# set our batch size (typically on most mid tier systems we'll use 16-32)\n",
        "batch_size = 32\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEWrhubk44ao"
      },
      "source": [
        "### Training out Model\n",
        "- Note we're using checkpointing and early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw4Hmyl744ao",
        "outputId": "73cf3c95-bfec-48e9-db30-d7305ee8a16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 10s 841ms/step - loss: 3.7882 - accuracy: 0.6928 - val_loss: 0.0776 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.07757, saving model to Model.h5\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 8s 745ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.07757 to 0.00295, saving model to Model.h5\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 8s 764ms/step - loss: 4.5759e-04 - accuracy: 1.0000 - val_loss: 2.4644e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00295 to 0.00025, saving model to Model.h5\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 8s 769ms/step - loss: 6.1044e-05 - accuracy: 1.0000 - val_loss: 5.8434e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00025\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 8s 795ms/step - loss: 3.4480e-05 - accuracy: 1.0000 - val_loss: 1.9160e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00025 to 0.00002, saving model to Model.h5\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 8s 757ms/step - loss: 7.9198e-06 - accuracy: 1.0000 - val_loss: 8.7518e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00002 to 0.00001, saving model to Model.h5\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 8s 849ms/step - loss: 4.1758e-06 - accuracy: 1.0000 - val_loss: 4.7518e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00001 to 0.00000, saving model to Model.h5\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 8s 852ms/step - loss: 1.4251e-06 - accuracy: 1.0000 - val_loss: 1.8063e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to Model.h5\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 8s 815ms/step - loss: 8.5899e-07 - accuracy: 1.0000 - val_loss: 1.1540e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to Model.h5\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 8s 794ms/step - loss: 6.4917e-07 - accuracy: 1.0000 - val_loss: 1.0530e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to Model.h5\n"
          ]
        }
      ],
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "                     \n",
        "checkpoint = ModelCheckpoint(\"Model.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "# We use a very small learning rate \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Enter the number of training and validation samples here\n",
        "nb_train_samples = 102 \n",
        "nb_validation_samples = 28 \n",
        "\n",
        "# We only train 5 EPOCHS \n",
        "epochs = 10\n",
        "batch_size = 1\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "#     steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator)\n",
        "#     validation_steps = nb_validation_samples // batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uritHTZQ44ao"
      },
      "source": [
        "### Loading our classifer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN9kOObG44ao"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "classifier = load_model('Model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uelQZ7_144ao"
      },
      "source": [
        "### Testing our classifer on some test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EJ3SOKVJ44ap",
        "outputId": "072b2ba5-d4ad-450b-f4f0-16ea64f9c708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amar\n",
            "Abhishek\n",
            "Abhishek\n",
            "Abhishek\n",
            "Amar\n",
            "Adithya Aras\n",
            "Abhishek\n",
            "Adithya Aras\n",
            "Akshay\n",
            "Amar\n",
            "Akshay\n",
            "Akshay\n",
            "Adithya Aras\n",
            "Adithya Aras\n",
            "Adithya Aras\n",
            "Akshay\n",
            "Akshay\n",
            "Akshay\n",
            "Abhishek\n",
            "Amar\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "def convert(l):\n",
        "    res_dict = {\"[\"+str(i)+\"]\": l[i] for i in range(0, len(l))}\n",
        "    return res_dict\n",
        "def convert2(l):\n",
        "    res_dict = {l[i]: l[i] for i in range(0, len(l))}\n",
        "    return res_dict\n",
        "\n",
        "faces_dict=convert(l)\n",
        "faces_dict_n=convert2(l)\n",
        "\n",
        "\n",
        "def draw_test(name, pred, im):\n",
        "    monkey = faces_dict[str(pred)]\n",
        "    BLACK = [0,0,0]\n",
        "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
        "    cv2.putText(expanded_image, monkey, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
        "    cv2.imshow(name, expanded_image)\n",
        "    print(monkey)\n",
        "    return monkey\n",
        "\n",
        "def getRandomImage(path):\n",
        "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
        "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
        "    random_directory = np.random.randint(0,len(folders))\n",
        "    path_class = folders[random_directory]\n",
        "#     print(\"Class - \" + faces_dict_n[str(path_class)])\n",
        "    file_path = path + path_class\n",
        "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
        "    random_file_index = np.random.randint(0,len(file_names))\n",
        "    image_name = file_names[random_file_index]\n",
        "    return cv2.imread(file_path+\"/\"+image_name)    \n",
        "\n",
        "for i in range(0,20):\n",
        "        input_im = getRandomImage(\"datasets/validation/\")\n",
        "        input_original = input_im.copy()#\n",
        "        input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
        "    \n",
        "        input_im = cv2.resize(input_im, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "        input_im = input_im / 255.\n",
        "        input_im = input_im.reshape(1,224,224,3) \n",
        "    \n",
        "        # Get Prediction\n",
        "        res = np.argmax(classifier.predict(input_im, 1, verbose = 0), axis=1)\n",
        "    \n",
        "        # Show image with predicted class\n",
        "        name=draw_test(\"Prediction\", res, input_original) \n",
        "    \n",
        "        cv2.waitKey(1000)\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Gk-dBxv944ap",
        "outputId": "57c72730-64a5-49df-b726-12f6564f0ea4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-43-04c88e74be86>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if faces is ():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Akshay\n"
          ]
        }
      ],
      "source": [
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "def face_extractor(img):\n",
        "    # Function detects faces and returns the cropped face\n",
        "    # If no face detected, it returns the input image\n",
        "\n",
        "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
        "    \n",
        "    if faces is ():\n",
        "        return None\n",
        "    \n",
        "    \n",
        "    # Crop all faces found\n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(img, (x, y),(x+w,y+h), (0,0,255), 1)\n",
        "        cropped_face = img[y:y+h, x:x+w]\n",
        "\n",
        "    return cropped_face\n",
        "count=0\n",
        "video_capture = cv2.VideoCapture(1)\n",
        "while(cv2.waitKey(1)<0):\n",
        "    count+=1\n",
        "    _, frame = video_capture.read()\n",
        "    face=face_extractor(frame)\n",
        "    if type(face) is np.ndarray:\n",
        "        face = cv2.resize(face, (224, 224))\n",
        "#         im = Image.fromarray(face, 'RGB')\n",
        "#         img_array = np.array(im)            \n",
        "#         img_array = np.expand_dims(img_array, axis=0)\n",
        "        \n",
        "        input_original = face.copy()\n",
        "        input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
        "    \n",
        "        face = cv2.resize(face, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "        face = face / 255.\n",
        "        face = face.reshape(1,224,224,3) \n",
        "        res = np.argmax(classifier.predict(face, 1, verbose = 0), axis=1)\n",
        "        name=draw_test(\"Prediction\", res, input_original) \n",
        "        cv2.waitKey(1000)\n",
        "        print(name)\n",
        "        cv2.putText(frame,name, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
        "    else:\n",
        "        cv2.putText(frame,\"There is no face in the frame\", (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
        "    cv2.imshow('Video', frame)\n",
        "    cv2.waitKey(3000)\n",
        "    if count==3:\n",
        "        break\n",
        "# cv2.destroyAllWindows()\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbxNBqY244ap"
      },
      "source": [
        "### Entering the Data onto a Spreadsheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9MiIqoJ44aq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSLCoa_844aq"
      },
      "outputs": [],
      "source": [
        "d = datetime.now()\n",
        "date=str(d)[0:10]\n",
        "time=str(d)[11:19]\n",
        "\n",
        "df=pd.read_excel(\"Attendence_8th_A.xls\")\n",
        "df=df.iloc[:,1:]\n",
        "if [name,date] in df.iloc[:,:2].values.tolist():\n",
        "    print(\"You are marked already as present for today\")\n",
        "\n",
        "else:\n",
        "    df=df.append({'Name':name,'Date':date,'Time':time},ignore_index=True)\n",
        "    df.to_excel(\"Attendence_8th_A.xls\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPgQ0CL-44aq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Facial Recognition using MobileNet Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}